\label{sec:tools}We shall first introduce much of the notation and background
material necessary for future discussions. First we introduce notation.
Following that, we shall review some previous results on Sturm-Liouville
theory and orthogonal polynomials. Finally, we introduce the topics
which will serve to lead into our discussion of the orthogonal rational
functions over the infinite interval.

\section{Notation}

\label{sec:tools-notation}In this section we lay out the notation
necessary for our exploration of the Wiener rational basis. In all future
sections we attempt to adhere to the conventions laid out here.

\subsection{Domains}

\label{sec:tools-notation-domains}We take $\mathbbm{R}$ and $\mathbbm{C}$ to
be the real and complex number fields, respectively. We shall consider the
following domains:
\begin{itemize}
  \item $\theta \in [- \pi, \pi]$
  
  \item $z \in \mathbbm{T} = \left\{ w \in \mathbbm{C} : |w| = 1 \right\}$
  
  \item $r \in I_r = [- 1, 1]$
  
  \item $x \in \mathbbm{R}$
\end{itemize}
We also make use of the subdomains:
\begin{itemize}
  \item $\theta \in I_{\theta} = [0, \pi]$
  
  \item $z \in \mathbbm{T}^+ = \left\{ w \in \mathbbm{T} : \tmop{Im} \{w\}
  \geq 0 \right\}$
  
  \item $x \in I_x = \mathbbm{R}^+ = [0, \infty]$
\end{itemize}
In all that follows, the variables $\theta, z, r,$ and $x$ will be reserved to
be associated with only their respectively domains above. Transformations
between these domains are not unique, but in Tables \ref{tab:xforms} and
\ref{tab:jacxforms} we list the main transformations that will be used
throughout this work. We will occasionally utilize transformations
that differ from those in the Tables, but in such situations the differnece
will be clear.

\begin{table}[tbp]
  \renewcommand{\arraystretch}{1.5}
  \begin{align*}\begin{array}{c|c|c|c|c|cc}
       & x & z & \theta & r &  & \\\cline{1-5}
       x & x \in [0, \infty] & z = - \frac{x - i}{x + i} & \theta = 
       2 \arctan x & r = \frac{1 - x^2}{1 + x^2} &  &
       \\\cline{1-5}
       z & x = i \frac{1 - z}{1 + z}  & z \in \mathbbm{T}^+ & \theta = \arg z
       & r = \frac{1}{2} \left( z + \bar{z} \right) &  & \\\cline{1-6}
       \theta & x = \tan \left( \frac{\theta}{2} \right) & z = e^{i \theta} &
       \theta \in [0, \pi] & r = \cos \theta &  & \\\cline{1-5}
       r & x = \sqrt{\frac{1 - r}{1 + r}} & z = e^{i \arccos r} & \theta =
       \arccos r & r \in [- 1, 1] &  & \\\cline{1-5}
     \end{array}\end{align*}
 
  \caption{Isomorphic transforms between different domains.}
  \label{tab:xforms}
\end{table}





\begin{table}[tbp]
  \renewcommand{\arraystretch}{1.5}
  \begin{center}
  \begin{tabular}{c|c|c|c|}
    $b \backslash a$ & $x$ & $\theta$ & $r$\\\cline{1-4}
    $x$ & 1 & $\frac{2}{1 + x^2}$ & $\frac{- 4 x}{(x^2 +
    1)^2}$\\\cline{1-4}
    $\theta$ & $\frac{1}{1 + \cos \nonesep \theta}$ & 1 & $- \sin
    \theta$\\\cline{1-4}
    $r$ & $\frac{- 1}{(1 + r) (1 - r^2)^{1 / 2}}$ & $- \frac{1}{\sqrt{1 -
    r^2}}$ & 1\\\cline{1-4}
  \end{tabular}
  \end{center}
  \caption[Jacobians for isomorphic transformations]{Jacobian relations for transformations. The table lists
  $\frac{\mathd a}{\mathd b}$ as a function of $b$.}
  \label{tab:jacxforms}
\end{table}  

All of the transformations in Table \ref{tab:xforms} are isomorphisms between
the respective domains listed. Table \ref{tab:jacxforms} lists the Jacobian
relations between different domains. There are a few remarks to make about
this. The transformation
\[ z = - \frac{x - i}{x + i}, \]
is a special kind of linear fractional/M$\ddot{\text{o}}$bius map and is
sometimes called the Cayley transform (up to complex conjugation and a phase).
It is a certain kind of linear fractional map which maps the extended upper
half-plane into the unit disk and is used extensively in the theory of
complex-valued orthogonal rational functions {\cite{bultheel1999}}. This
technique of using a linear fractional map to transform the boundary of the
unit disk to the real line shall be used extensively later.



The transformation
\[ r = \cos \theta, \]
commonly appears in the theory of orthogonal polynomials {\cite{szego1975}}.
It relates the Chebyshev polynomials to the Fourier basis functions and is
vital in the application of the fast Fourier Transform algorithm to polynomial
approximation methods. In general, it relates families of polynomials on the
interval $I_r = [- 1, 1]$ to the Szeg$\ddot{\text{o}}$ polynomials
{\cite{szego1975}} on the upper unit circle.



We shall sometimes avoid utilizing the explicit expressions listed above and
instead we use transformations for them. In particular, the independent
variables $x$, $r$, $\theta$, and $z$ will often be written as functions of
one another. Conforming to the transformations presented in Table
\ref{tab:xforms} above, we have e.g.
\[ \begin{array}{lll}
     x (r) & = &  \sqrt{\frac{1 - r}{1 + r}}\\
     &  & \\
     x (\theta) & = & \tan \frac{\theta}{2}\\
     &  & \\
     r (\theta) & = & \cos \theta\\
     &  & \\
     r (x) & = & \frac{1 - x^2}{1 + x^2}
   \end{array} \]
In later sections, we will augment the relationship between $x$, $r$, and
$\theta$, and the above relations will change. Because of this, we shall only
use the notation e.g. $r (x)$ when it is clear what the relationship between
$r$ and $x$ is.



\subsection{Function spaces}

\label{sec:tools-notation-fspaces}By $L^2 (A, B ; \mu)$ we mean the space of
square-integrable functions under the positive measure $\mu$ mapping $A
\subset \mathbbm{R}$ to $B \subset \mathbbm{F}$. In this work, we shall always
have $\mu$ absolutely continuous with respect to Lebesgue measure so that a
density, which we call $w (\cdot)$, exists almost everywhere and
the associated $L^2$ space is $L^2_w (A, B) = L^2 (A, B ; w)$. If $w \equiv
1$, we shall often omit writing it, and we also omit either $B$ or both
$A$ and $B$ if the context is clear.



Being a Hilbert space, we endow $L^2_w (A, B)$ with the conjugate bilinear
inner product
\[ \langle f, g \rangle_w = \int_A f \bar{g} w \mathd x, \]
whose associated norm completes the space. If $B$ is a subset of
$\mathbbm{R}$, this reverts to the (non-conjugate) bilinear form.



We shall infrequently make reference to the Sobolev space 
\begin{align*}
H^s = \left\{ f :f^{(j)} \in L^2 (A, B), j = 0, 1, \ldots s \right\},
\end{align*}

where $f^{(j)}$ denotes the $j$th derivative, and $f^{(0)} \equiv f$. For
$s$ an integer, the norm is defined as

\[ \| \rho \|^2_{H^s} = \sum_{q = 0}^s \left| \left| \frac{\mathd^q
   \rho}{\mathd \nonesep r^q} \right| \right|^2_{L^2} = \sum_{q = 0}^s \int_A
   \left( \frac{\mathd^q \rho}{\mathd r^q} \right)^2 \mathd r. \]
It is also possible to define real-valued Sobolev spaces; one may
either use the continuous Fourier transform or it can be done via the concept
of complex interpolation {\cite{triebel1995}}. The Sobolev space parameter $s$
is intricately tied with spectral expansion convergence rates for orthogonal
polynomials {\cite{canuto1982}} and Fourier series {\cite{hesthaven2007}}.



There are specific weight functions $w$ that will play central roles in this
work. The following weight functions are often-used:
\[ \begin{array}{lll}
     w_r^{(\alpha, \beta)} & = & (1 - r)^{\alpha} (1 + r)^{\beta}\\
     &  & \\
     w_{\theta}^{(\gamma, \delta)} & = & w_r^{(\delta, \gamma)} = (1 + \cos
     \theta)^{\gamma} (1 - \cos \theta)^{\delta}\\
     &  & \\
     w_x^{(s, t)} & = & w_{\theta}^{(s, t)} = \frac{2^{s + t}}{(1 + x^2)^s} 
     \left( \frac{x^{2 t}}{(1 + x^2)^t} \right) .
   \end{array} \]
We have defined these weights so that e.g. $w_x^{(s, t)} (x (r)) = w_r^{(t,
s)} (r)$. The motivation for the order of the indices in the weight functions
is to mimic the orientation of the Jacobi weight $w_r^{(\alpha, \beta)}$, i.e.,
$\alpha$ is the exponent on the right-hand zero $(1 - r)$, and $\beta$ is the
exponent on the left-hand weight $(1 + r)$. Thus, $(\alpha, \beta)$, $(\gamma,
\delta)$, and $(s, t)$ correspond to the order of the root at the (right,left)
endpoints on $I_r$, $I_{\theta}$, and $I_x$, respectively.



All of the above weights are semi-positive. However, it will be common for us to
take non-principal branches when we take square roots. When this is the case,
we'll denote this particular square root as $\sqrt[\asterisk]{\cdot}$. In
particular, for the weights given above, we take the following non-principal
square roots:
\begin{equation}
  \label{eq:weight-np} \begin{array}{lll}
    \sqrt[\asterisk]{w_r^{(\alpha, \beta)}} & = & \sqrt{w_r^{(\alpha, \beta)}}
    = (1 - r)^{\alpha / 2} (1 + r)^{\beta / 2}\\
    &  & \\
    \sqrt[\asterisk]{w_{\theta}^{(\gamma, \delta)}} & = & 2^{\left(
    \frac{\gamma + \delta}{2} \right)} e^{i \left( \frac{\gamma + \delta}{2}
    \right) \theta} \sin^{\delta} \left( \frac{\theta}{2} \right)
    \cos^{\gamma} \left( \frac{\theta}{2} \right)\\
    &  & \\
    \sqrt[\asterisk]{w_x^{(s, t)}} & = & \frac{2^{\left( \frac{s + t}{2}
    \right)} x^t}{(x - i)^{s + t}} .
  \end{array}
\end{equation}
We will also make use of the principal-branch square roots $\sqrt{\cdot}$,
which will have obvious value since all the weight functions are non-negative.
The non-principal branch square roots satisfy the property
\[ \sqrt[\asterisk]{w}  \overline{\sqrt[\asterisk]{w}} = w. \]




On $L^2 (I_r, \mathbbm{R} ; w)$ we define polynomial projection
operators $\mathcal{P}^w_N$. For the weight $w$, there exist a family of
orthonormal polynomials $\left\{ p_n (r) \right\}_{n = 0}^{\infty}$. We shall
consider expansions of $\rho \in L^2_w (I_r, \mathbbm{R})$ in these
polynomials
\[ \rho = \sum_{n = 0}^{\infty} \hat{\rho}_n p_n, \]
where equality holds in the $L^2_w$ norm. The $\hat{\rho}_n$ are defined by
\[ \hat{\rho}_n = \frac{\left\langle \rho, p_n \right\rangle_w}{\|p_n \|^2_w}
\]
That such an expansion exists for certain $w$ can be deduced by considering
the appropriate self-adjoint singular Sturm-Liouville eigenvalue problems
associated with e.g. the Jacobi polynomials, and then applying the spectral
theorem. Associated with the polynomials, we define the polynomial subspace
$\mathcal{B}_N = \tmop{span} \left\{ r^n : 0 \leq n \leq N - 1 \right\}$. We
define the projection operator $\mathcal{P}^w_N$ on $L^2_w (I_r, \mathbbm{R})$
as
\[ \mathcal{P}^w_N \rho = \sum_{n = 0}^N \hat{\rho}_n p_n . \]
Thus, $\mathcal{P}^w_N$ is the $L^2_w (I_r, \mathbbm{R})$-orthogonal
projection onto $\mathcal{B}_N$. And it satisfies
\[ \left\langle \mathcal{P}^w_N \rho - \rho, \phi \right\rangle_w = 0,
   \hspace{0.5cm} \forall \phi \in B_N \]


On $L_w^2 ([- \pi, \pi], \mathbbm{C})$ or $L^2_w (I_{\theta}, \mathbbm{C})$ we
define the space
\[ \mathcal{T}_N = \tmop{span} \left\{ e^{in \theta} : |n| \leq N - 1
   \right\}, \]
and for any $\Theta \in L^2_w \left( [- \pi, \pi], \mathbbm{C} \right)$ which
is Fourier-expandable as
\[ \Theta = \sum_{n \in \mathbbm{Z}} \hat{\Theta}_n e^{in \theta} \]
the associated projection operator is
\[ \mathcal{P}^{\theta, w}_N \Theta = \sum_{|n| < N} \hat{\Theta}_n e^{in
   \theta}, \]
so that again $\mathcal{P}^{\theta, w}_N$ is the $L^2_w ([- \pi, \pi],
\mathbbm{C})$-orthogonal projection onto $\mathcal{T}_N$.

\section{The Sturm-Liouville eigenvalue problem, Jacobi polynomials}

\label{sec:tools-jacobi}For an in-depth survey of the theory in this field, we
point the reader to {\cite{slp:zettl2005}}, {\cite{szego1975}},
{\cite{gautschi2004}}, and {\cite{abramowitz1972}}. We begin by presenting the
canonical second-order equation
\begin{equation}
  \label{eq:slp-canonical} \begin{array}{ll}
    - \frac{\mathd}{\mathd x} \left[ p (x) y' \right] + q (x) y - \lambda w
    (x) y = 0, & x \in I
  \end{array}
\end{equation}
If we are to be precise, we also require some associated boundary conditions.
The functions $p (x)$, $q (x)$, and $w (x)$ must obey certain regularity
conditions.



Under these conditions, in conjunction with the spectral theorem, then
equation (\ref{eq:slp-canonical}) admits a discrete spectrum $\left\{
\lambda_n \right\}$ whose associated eigenfunctions $\left\{ y_n \right\}$
form a complete basis for $L^2 (I, \mathbbm{R} ; w)$ and are orthogonal under
the weight $w (x)$. For polynomial solutions to a Sturm-Liouville problem one
may consider the 
Jacobi differential equation
\begin{equation}
  \label{eq:slp-jacobi} \begin{array}{ll}
    (1 - r^2) \rho'' + \left[ \beta - \alpha - (\alpha + \beta + 2) r \right]
    \rho' + n (n + \alpha + \beta + 1) \rho = 0, & r \in [- 1, 1],
  \end{array}
\end{equation}
for $\alpha, \beta > - 1$ and any $n \in \mathbbm{N}_0$. The associated
polynomial eigenfunctions, which depend on the values of $\alpha$ and $\beta$
are called Jacobi polynomials, and are standard classical orthogonal
polynomials. The monic Jacobi polynomial of order $n$, as a solution to
(\ref{eq:slp-jacobi}), is written as $P^{(\alpha, \beta)}_n (r)$. For any
$\alpha, \beta > - 1$, the Jacobi polynomials of class $(\alpha, \beta)$ are
orthogonal under a weighted $L^2$ inner product:
\begin{equation}
  \label{eq:orthogonality} \int_{- 1}^1 P^{(\alpha, \beta)}_m P_n^{(\alpha,
  \beta)} (1 - r)^{\alpha} (1 + r)^{\beta} \mathd r = h^{(\alpha, \beta)}_n
  \delta_{m, n},
\end{equation}
where $\delta_{m, n}$ is the Kronecker delta function, and $h_n^{(\alpha,
\beta)}$ is a normalization constant given in Appendix
\ref{app:polynomials-jacobi}. These orthogonal polynomials form the foundation
for a large portion of this work. We shall usually deal with the orthonormal
Jacobi polynomials $\tilde{P}^{(\alpha, \beta)}_n (r)$ which satisfy the
relation
\[ \int_{- 1}^1 \tilde{P}_n^{(\alpha, \beta)} (r) \tilde{P}_m^{(\alpha,
   \beta)} (r) w_r^{(\alpha, \beta)} \mathd r = \delta_{m, n} . \]
It will be common for use to use a tilde ($\widetilde{ }$) above a function to
denote that it is normalized with respect to the $L^2$ norm under which the
functions are orthogonal. The orthonormal Jacobi polynomials
$\tilde{P}^{(\alpha, \beta)}_n$ will be integral in the derivation of the
Wiener rational function basis on the real line. All of the various
properties of Jacobi polynomials that we will use at some point are collected
in Appendix \ref{app:polynomials-jacobi}. We require a minor generalization of
Jacobi polynomials: we perform a change of dependent variable in 
(\ref{eq:slp-jacobi}) to obtain the following:



\begin{lemma}
  \label{lemma:jacobi-functions}(Jacobi Functions)
  
  The Jacobi functions defined as
  \[ P^{(\alpha, \beta, a, b)}_n (r) = (1 - r)^a (1 + r)^b P_n^{(\alpha,
     \beta)} (r) \]
  satisfy the following properties:
  \begin{enumerate}
    \item $\left\{ P^{(\alpha, \beta, a, b)}_n (r) \right\}_{n \in
    \mathbbm{N}_0}$ are orthogonal and complete in $L^2 (I_r, \mathbbm{R} ;
    w)$ under the weight $w (r) = (1 - r)^{\alpha - 2 a} (1 + r)^{\beta - 2
    b}$.
    
    \item The $P^{(\alpha, \beta, a, b)}_n (r)$ are eigenfunctions $\rho_n
    (r)$ of the Sturm-Liouville problem
    \begin{equation}
      \label{eq:slp-jacobi-functions} - \frac{\mathd}{\mathd r} \left[ p (r)
      \rho' (r) \right] + q (r) \rho (r) - \lambda w (r) \rho (r) = 0,
    \end{equation}
    with the parameters
    \begin{equation}
      \left. \label{eq:slp-jacobi-general-coeffs} \begin{array}{lll}
        p (r) & = & (1 - r)^{\alpha + 1 - 2 a} (1 + r)^{\beta + 1 - 2 b} \\
        &  & \\
        q (r) & = & \left[ a (\alpha - a) (1 - r)^{- 2} + b (\beta - b) (1 +
        r)^{- 2} \right] (1 - r)^{\alpha + 1 - 2 a} (1 + r)^{\beta + 1 - 2
        b}\\
        &  & \\
        w (r) & = & (1 - r)^{\alpha - 2 a} (1 + r)^{\beta - 2 b}\\
        &  & \\
        \lambda_n & = & n (n + \alpha + \beta + 1) - 2 ab + a (\beta + 1) + b
        (\alpha + 1)
      \end{array} \right\}
    \end{equation}
  \end{enumerate}
\end{lemma}

The proof of Lemma \ref{lemma:jacobi-functions} is simple but tedious. It 
relies on the ability to compute an appropriate integrating factor to transform
equation (\ref{eq:slp-jacobi}) into (\ref{eq:slp-jacobi-functions}). Note that
if one takes $a = b = 0$ in Lemma (\ref{lemma:jacobi-functions}), the
resulting problem and eigensolutions revert back to the Jacobi polynomials, as
expected. We shall often write $\text{$P_n^{(\alpha, \beta, \alpha / 2, \beta
/ 2)}$=$p_n^{(\alpha, \beta)}$}$, i.e., $p_n^{(\alpha, \beta)}$ are the Jacobi
polynomials $P_n^{(\alpha, \beta)}$ multiplied by the square root of their
associated weight function, and thus these functions are orthogonal under the
unweighted $L^2$ inner product. We also define tilde'd versions as the
normalized functions: $\tilde{p}_n^{(\alpha, \beta)} = \sqrt{w_r^{(\alpha,
\beta)}}  \tilde{P}_n^{(\alpha, \beta)}$. Due to the previous lemma, these
Jacobi functions $\tilde{p}_n^{(\alpha, \beta)}$ are orthonormal under
unweighted Lebesgue measure.

The above discussion has centered mainly around the class of Jacobi orthogonal
polynomials, but there are other classes of orthogonal polynomials that are
relevant to our future tasks. In particular, the very popular Legendre
polynomials and Chebyshev polynomials (of the first, second, third, and fourth
kinds) are subsets of the Jacobi orthogonal polynomials. In addition, we will
make use of the Laguerre and Hermite polynomials. Because these two latter
families are only for comparison purposes, we will not expound on their
properties as much as with the Jacobi case, but we do include basic
information about these polynomial families in Table
\ref{tab:orthogonal-polynomials}, and more in-depth information can be found
in Chapter \ref{sec:other} and Appendix \ref{app:polynomials}.



\begin{table}[tbp]
  \renewcommand{\arraystretch}{1.5}
  \begin{center}
  \begin{tabular}{cr|c|c|}
    &  & Domain & $w$\\\cline{3-4}
    & Jacobi & $r \in [- 1, 1]$ & $(1 - r)^{\alpha} (1 + r)^{\beta}$,
    $\alpha, \beta > - 1$\\\cline{3-4}
    & Laguerre & $x \in [0, \infty)$ & $x^{\alpha} e^{- x}$, $\alpha > -
    1$\\\cline{3-4}
    & Hermite & $x \in \mathbbm{R}$ & $|x |^{2 \alpha} e^{- x^2}$, $\alpha >
    - \frac{1}{2}$\\\cline{3-4}
  \end{tabular}
  \end{center}
  \caption[Weight functions for orthogonal polynomial families]{Classical orthogonal polynomials: their integral weights and
  domains. See appendix \ref{app:polynomials} for the recurrence coefficients.
  }
  \label{tab:orthogonal-polynomials}
\end{table}



In all future sections we make extensive use of the notation that a
tilde'd function is the normalized version (with respect to some norm), and a
lowercase letter (Roman or Greek) is the uppercase function multiplied by the
(perhaps non-principal) square root of the associated weight.

\subsection{Polynomial Quadrature}

\label{sec:tools-jacobi-quadrature}In this section we review some basic
theoretical and computational results for orthogonal polynomials. Although the
results here are applicable to any collection of general orthogonal
polynomials, much of this work will only apply these results to the Jacobi
polynomial case.



We adopt the following notation for quadrature: a quadrature rule $Q^{}_N (x,
\omega)$ is defined as the linear operator defined by $N$ nodal points
$\left\{ x_j \right\}_{j = 1}^N$ and weights $\left\{ \omega_j \right\}_{j =
1}^N$, which acts on a function $f$:


\begin{equation}
  \label{eq:quadrule-def} Q_N (x, \omega) \left[ f \right] = \sum_{j = 1}^N
  \omega_j f (x_j) .
\end{equation}
Quadrature rules are used as approximations of integrals against certain
weight functions. A very special type of quadrature rule is one that is
Gaussian:

\begin{definition}
  (Gaussian quadrature)
  
  Given a collection of functions $\left\{ \phi_n (x) \right\}_{n =
  1}^{\infty}$ defined over some interval $x \in I$ and a weight function $w
  (x)$. The $N$-point quadrature rule $Q_N (x_n, \omega_n) [\cdot]$ is called
  Gaussian if
  \[ \int_I \phi_n (x) w (x) \mathd x = \sum_{m = 1}^N \phi_n (x_m) \omega_m
     \assign Q_N (x_i, \omega_i) [\phi_n], \hspace{1cm} n = 1, 2, \ldots, 2 N.
  \]
\end{definition}



The following two results are classical well-known results in the theory of
orthogonal polynomials:



\begin{lemma}
  All families of polynomials orthogonal with respect to some weight $w (x)$
  admit a three-term recurrence relation of the form
  \begin{equation}
    \label{eq:monic-opoly-recurrence} p_{n + 1} (x) = (x - a_n) p_n (x) - b_n
    \nonesep p_{n - 1} (x), n > 1
  \end{equation}
  for constants $a_n$ and $b_n$. 
\end{lemma}

The nodal locations of the $N$-point Gauss quadrature rule for an orthogonal polynomial
family $P_n$ are given by the roots of the polynomial $P_N$. This, along with 
the existence of a recurrence relation for orthogonal polynomials allows us to
easily compute the Gaussian quadrature rule {\cite{golub1969}}:

\begin{theorem}
  \label{thm:gauss-quadrature}The Gaussian quadrature nodes and weights $(r_n,
  \omega_n)_{n = 1}^N$ for the polynomials $P^{(\alpha, \beta)}_n$ are given
  as eigenvalues and components of eigenvectors of a symmetric tridiagonal
  matrix. Define
  \[ J = \left[ \begin{array}{lllllll}
       a_0 & \sqrt{b_1} & 0 & 0 & \cdots &  & 0\\
       &  &  &  &  &  & \\
       \sqrt{b_1} & a_1 & \sqrt{b_2} & 0 & \cdots &  & 0\\
       &  &  &  &  &  & \\
       0 & \ddots & \ddots & \ddots &  &  & \\
       &  &  &  &  &  & \sqrt{b_{N - 1}}\\
       &  &  &  &  &  & \\
       0 &  & \cdots &  & 0 & \sqrt{b_{N - 1}} & a_{N - 1}
     \end{array} \right] . \]
  Let $\left\{ \lambda_n \right\}_{n = 1}^N$ be the eigenvalues of $J$ and
  $\left\{ v_n \right\}_{n = 1}^N$ be the orthonormalized eigenvectors of $J$.
  Then $r_n = \lambda_n$ and $\omega_n = b_0 v^2_{n, 1}$ for $n = 1, \ldots,
  N$.
\end{theorem}

For a very special case of the Jacobi polynomials, we can find an explicit
formula for the Gauss quadrature nodes and weights:

\begin{lemma}
  \label{lemma:chebyshev-gauss-quadrature}The Gaussian quadrature $\left\{
  (r_n, \omega_n) \right\}_{n = 1}^N$ for the Chebyshev polynomials $\left(
  \text{i.e. the polynomials \ } \tilde{P}^{(- 1 / 2, - 1 / 2)}_n (r) \right)$
  are cosine maps of equidistant nodes. Define
  \[ \theta_n = \frac{(2 n + 1) \pi}{2 N} . \]
  Then
  \[ \begin{array}{lll}
       r_n & = & - \cos \theta_n\\
       &  & \\
       \omega_n & = & \frac{\pi}{N} .
     \end{array} \]
\end{lemma}



Theorem \ref{thm:gauss-quadrature} allows us to easily compute Gaussian
quadrature formulae for any collection of orthogonal polynomials provided that
we know the recurrence relation. This shows how powerful the recurrence
constants $a_n$ and $b_n$ are: they allow us not only to evaluate the
polynomials via (\ref{eq:monic-opoly-recurrence}), but they also
allow us to perform accurate (Gaussian) quadrature. There are, of course,
other kinds of quadrature rules. In particular, Clenshaw-Curtis quadrature
{\cite{clenshaw1960}} is a popular alternative, and is in many cases 
competitive with Gaussian quadrature {\cite{trefethen2008}}. One of the
much-touted advantages of Clenshaw-Curtis quadrature is that it can be
implemented via the fast Fourier transform (FFT). However, we will show in
Chapter \ref{sec:jacobifft} that Gaussian-like quadrature can also be
implemented with the FFT for relatively general values of the parameters
$(\alpha, \beta)$.



It is already well-known that for $(\alpha, \beta) = \left( - \frac{1}{2}, -
\frac{1}{2} \right)$, the Gaussian quadrature rule for Jacobi polynomials
(i.e. the Chebyshev polynomials) can be implemented via the FFT. This is
evident from the result of Lemma \ref{lemma:chebyshev-gauss-quadrature}, since
the Chebyshev-Gauss quadrature is just a Fourier quadrature in $\theta$-space.
We will present a similar FFT result for a greater variety of parameter
families $(\alpha, \beta)$.



We'll denote $\left\{ \left( r_n^{(\alpha, \beta)}, \omega_n^{(\alpha,
\beta)} \right) \right\}_{n = 1}^N$ the $N$-point nodes and weights for the
Jacobi polynomial $(\alpha, \beta)$ Gaussian quadrature rule. One of the major
reasons that we require a quadrature rule is our desire to determine the
spectral coefficients of the expansion
\[ \begin{array}{lll}
     \mathcal{P}_N^{w_r^{(\alpha, \beta)}} f (r) & = & \sum_{n = 0}^{N - 1}
     \hat{f}_n  \tilde{P}_n^{(\alpha, \beta)} (r),
   \end{array} \]
where
\[ \hat{f}_n = \left\langle f, \tilde{P}_n^{(\alpha, \beta)} \right\rangle =
   \int_{- 1}^1 f (r) \tilde{P}_n^{(\alpha, \beta)} (r) w_r^{(\alpha, \beta)}
   (r) \nonesep \mathd r \simeq Q_N \left( r_n^{(\alpha, \beta)},
   \omega_n^{(\alpha, \beta)} \right) \left[ f \tilde{P}^{(\alpha, \beta)}_n
   \right] . \]
Thus, quadrature rules give us an accurate way to compute the modal
coefficients for a spectral expansion given the nodal values $f \left(
r_n^{(\alpha, \beta)} \right)$. The error made in using the quadrature rule
$Q_N$ to approximate the modal coefficient integral in lieu of computing the
actual value is often called the \tmtextit{aliasing error}.



Other Gauss-type quadrature rules also exist for polynomials, including
Gauss-Radau, Gauss-Lobatto, and Gauss-Kronrod rules. We introduce the Radau
and Lobatto rules below, both of which will later use.

\begin{definition}
  (Gauss-Radau Quadrature)
  
  Given a collection of functions $\left\{ \phi_n (x) \right\}_{n =
  1}^{\infty}$ defined over some interval $x \in I$, a fixed point $x_1 \in
  I$, and a weight function $w (x)$. The $N$-point quadrature rule $Q_N (x_n,
  \omega_n) [\cdot]$ is called Gauss-Radau if:
  \[ \int_I \phi_n (x) w (x) \mathd x = \phi_n (x_1) \omega_1 + \sum_{m = 2}^N
     \phi_n (x_m) \omega_m \assign Q_N (x_i, \omega_i) [\phi_n], \hspace{1cm}
     n = 1, 2, \ldots, 2 \nonesep N - 1. \]
\end{definition}

Gauss-Radau quadrature is the integration rule that results when we attempt a
Gauss-like rule but fix a single quadrature node $x_1$. Due to this
constraint, we pay a price in accuracy: we can only correctly integrate $2 N -
1$ polynomials compared with the $2 N$ polynomials in pure Gauss quadrature.



Our use of Gauss-Radau quadrature will be limited to Jacobi polynomials, and
although we are free to choose the fixed node $r_1$ to be any point in $[- 1,
1]$, we will always choose it to be $r_N = + 1$.

\begin{definition}
  (Gauss-Lobatto Quadrature)
  
  Given a collection of functions $\left\{ \phi_n (x) \right\}_{n =
  1}^{\infty}$ defined over some finite interval $x \in I$, the two endpoints
  $x_1$ and $x_N$ of $I$, and a weight function $w (x)$. The $N$-point
  quadrature rule $Q_N (x_n, \omega_n) [\cdot]$ is called Gauss-Lobatto if for
  all $n = 1, 2, \ldots, 2 N-2$:
  \[ \int_I \phi_n (x) w (x) \mathd x = \phi_n (x_1) \omega_1 + \phi_n (x_N)
     \omega_N + \sum_{m = 2}^{N - 1} \phi_n (x_m) \omega_m \assign Q_N (x_i,
     \omega_i) [\phi_n] \]
\end{definition}



Again our usage of Gauss-Lobatto rules will be restricted to the Jacobi
polynomial case where we will always take $r_1 = - 1$ and $r_N = + 1$. As
noted in {\cite{gautschi1999}}, the unknown nodes and weights for
Gauss-Radau/Lobatto quadrature, like Gauss quadrature, can be computed as
eigenvalues and eigenvector components of a symmetric tridiagonal matrix, i.e,
an analogue of Theorem \ref{thm:gauss-quadrature} exists for
Gauss-Radau/Lobatto quadrature rules, but we shall not present it. The reader
is referred to {\cite{gautschi1999}} and {\cite{gautschi2000}} for the
algorithm, which also contains algorithms for Gauss-Kronrod quadrature. Some
theory is presented in {\cite{milovanovic2006}}.



For notation, we will use $\left\{ r_n^{(\alpha, \beta)}, \omega \nonesep_{r,
n}^{(\alpha, \beta)} \right\}_{n = 1}^N$ to denote the Gauss-quadrature nodes
and weights for the Jacobi polynomials of order $(\alpha, \beta)$. Similarly,
$\left\{ r_n^{(\alpha, \beta) ; \tmop{GR}}, \omega_{r, n}^{(\alpha, \beta) ;
\tmop{GR}} \right\}_{n = 1}^N$ denotes the Gauss-Radau quadrature nodes and
weights for the Jacobi polynomials of order $(\alpha, \beta)$, and $\left\{
r_n^{(\alpha, \beta) ; \tmop{GL}}, \omega_{r, n}^{(\alpha, \beta) ; \tmop{GL}}
\right\}_{n = 1}^N$ denotes the Lobatto rule. Regrettably, the subscript $r$
on the quadrature weight $\omega$ is necessary to distinguish this weight from
others we shall define later. In addition, we confess our sloppiness in
notation: $\left\{ r_n^{(\alpha, \beta)} \right\}_{n = 1}^N$ and $\left\{
r_n^{(\alpha, \beta)} \right\}_{n = 1}^{N + 1}$ refer to two completely
different sets of nodes; i.e. $r_1^{(\alpha, \beta)}$ from the first set has
no relation to $r^{(\alpha, \beta)}_1$ from the second set. We have omitted
denoting the dependence on $N$ to keep notation as understandable as possible
for our purposes.







\section{Basic Approximation Theory}

\label{sec:tools-approx}We transcribe some theorems relating to polynomial
approximation theory here. To faciliate notation, we assume that for any space
$L^2_w (A, B)$, there exists some collection of polynomial basis functions
$\left\{ p_n \right\}_{\left. n \in \mathbbm{N \cup \left\{ 0 \right.} 0
\right\}}$. For example, if $w \equiv 1$, $A = [- 1, 1]$, and $B =
\mathbbm{R}$, then $p_n$ may be the Legendre polynomials, where we assume
that the index of the basis functions is directly related to the degree of the
polynomial function. We start off by presenting the following result
{\cite{canuto1982}}, {\cite{hesthaven2007}}:

\begin{lemma}
  \label{lemma:poly-approx}Suppose $u \in H^p_{w^{(\alpha, \beta)}_r} (I_r,
  \mathbbm{R})$ with $| \alpha | < 1$ and $| \beta | < 1$. There exists a
  constant $C$ for all $0 \leq q \leq p$
  \[ \|u - \mathcal{P}^{w_r^{(\alpha, \beta)}}_N u\|_{H^q_{w^{(\alpha,
     \beta)}_r} (I_r, \mathbbm{R})} \leq CN^{f (q) - p} ||u\|_{H^p_w (I_r,
     \mathbbm{R})}, \]
  where $f (q)$ is defined by
  \[ f (q) = \left\{ \begin{array}{ll}
       \frac{3}{2} q, & 0 \leq q \leq 1\\
       & \\
       2 q - \frac{1}{2}, & q \geq 1
     \end{array} \right. \]
\end{lemma}



The upshot of the Lemma is that if $u$ is not infinitely smooth, then
one can only expect algebraic convergence in $N$. However, if $u$ very smooth,
then one can expect spectral, or exponential, convergence of the polynomial
projection to the function in some Sobolev norm. Of course, if one requires
convergence in a high-degree Sobolev norm (large $q$), one must pay some
algebraic price ($f (q)$ increases as $q$ increases).



Similar results exist for Fourier expansions. We will not present any novel
approximation theory results, but we present this to give a flavor of what we
expect from any new basis set that we derive: convergence rate commensurate
with function regularity.
